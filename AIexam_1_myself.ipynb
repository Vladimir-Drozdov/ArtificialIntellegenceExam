{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ed6a2ac-9dd8-4349-a8e7-f980d00b1908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Эпоха 0/10000, Ошибка: 0.1260\n",
      "Эпоха 1000/10000, Ошибка: 0.0848\n",
      "Эпоха 2000/10000, Ошибка: 0.0473\n",
      "Эпоха 3000/10000, Ошибка: 0.0397\n",
      "Эпоха 4000/10000, Ошибка: 0.0369\n",
      "Эпоха 5000/10000, Ошибка: 0.0354\n",
      "Эпоха 6000/10000, Ошибка: 0.0345\n",
      "Эпоха 7000/10000, Ошибка: 0.0339\n",
      "Эпоха 8000/10000, Ошибка: 0.0335\n",
      "Эпоха 9000/10000, Ошибка: 0.0332\n",
      "\n",
      "Обучение завершено!\n",
      "Веса первого слоя:\n",
      " [[-3.28205576 -4.70142247 -4.70142157]\n",
      " [ 6.54479488 -5.56905648 -5.56905158]]\n",
      "Смещения первого слоя:\n",
      " [ 4.5705882  -1.17951146]\n",
      "Веса второго слоя:\n",
      " [[-6.02124468  8.06175343]]\n",
      "Смещения второго слоя:\n",
      " [-0.01390131]\n",
      "\n",
      "Прогнозы:\n",
      "Вход: [0 0 0], Выход: 0.0167, Ожидаемый выход: 0\n",
      "Вход: [0 0 1], Выход: 0.0563, Ожидаемый выход: 0\n",
      "Вход: [0 1 0], Выход: 0.0563, Ожидаемый выход: 0\n",
      "Вход: [0 1 1], Выход: 0.4846, Ожидаемый выход: 1\n",
      "Вход: [1 0 0], Выход: 0.9641, Ожидаемый выход: 1\n",
      "Вход: [1 0 1], Выход: 0.9682, Ожидаемый выход: 1\n",
      "Вход: [1 1 0], Выход: 0.9682, Ожидаемый выход: 1\n",
      "Вход: [1 1 1], Выход: 0.5023, Ожидаемый выход: 0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"Сигмоидальная функция активации.\"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    \"\"\"Производная сигмоиды.\"\"\"\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "def forward_pass(X, W1, b1, W2, b2):\n",
    "    \"\"\"Прямой проход через сеть.\"\"\"\n",
    "    Z1 = np.dot(X, W1.T) + b1\n",
    "    H = sigmoid(Z1)\n",
    "\n",
    "    Z2 = np.dot(H, W2.T) + b2\n",
    "    Y = sigmoid(Z2)\n",
    "\n",
    "    return Y, H, Z1\n",
    "\n",
    "def loss(y_pred, y_true):\n",
    "    \"\"\"Функция потерь (MSE).\"\"\"\n",
    "    return 0.5 * np.mean((y_pred - y_true)**2)\n",
    "\n",
    "def loss_derivative(y_pred, y_true):\n",
    "    \"\"\"Производная функции потерь (MSE).\"\"\"\n",
    "    return y_pred - y_true\n",
    "\n",
    "def backward_pass(X, Y_pred, Y_true, H, Z1, W1, b1, W2, b2, lr):\n",
    "    \"\"\"Обратный проход и обновление весов.\"\"\"\n",
    "    # Производные\n",
    "    d_L_d_Y = loss_derivative(Y_pred, Y_true)\n",
    "    d_Y_d_Z2 = sigmoid_derivative(np.dot(H, W2.T))\n",
    "    d_Z2_d_W2 = H\n",
    "    d_Z2_d_b2 = 1\n",
    "    d_Y_d_b2 = d_Y_d_Z2 * d_Z2_d_b2\n",
    "\n",
    "    d_L_d_Z2 = d_L_d_Y * d_Y_d_Z2\n",
    "\n",
    "    d_L_d_W2 = np.dot(d_L_d_Z2.T, d_Z2_d_W2)\n",
    "    d_L_d_b2 = np.sum(d_L_d_Z2, axis=0)\n",
    "\n",
    "\n",
    "    d_H_d_Z1 = sigmoid_derivative(Z1) # Исправлено\n",
    "    d_Z1_d_W1 = X\n",
    "    d_Z1_d_b1 = 1\n",
    "\n",
    "    d_Z2_d_H = W2\n",
    "    d_L_d_H = np.dot(d_L_d_Z2, d_Z2_d_H)\n",
    "\n",
    "    d_L_d_Z1 = d_L_d_H * d_H_d_Z1\n",
    "\n",
    "    d_L_d_W1 = np.dot(d_L_d_Z1.T, d_Z1_d_W1)\n",
    "    d_L_d_b1 = np.sum(d_L_d_Z1, axis=0)\n",
    "\n",
    "    # Обновление весов\n",
    "    W1 -= lr * d_L_d_W1\n",
    "    b1 -= lr * d_L_d_b1\n",
    "    W2 -= lr * d_L_d_W2\n",
    "    b2 -= lr * d_L_d_b2\n",
    "\n",
    "    return W1, b1, W2, b2\n",
    "\n",
    "\n",
    "def train(X, Y, lr = 0.01, epochs = 10000):\n",
    "    \"\"\"Обучение персептрона.\"\"\"\n",
    "    input_size = X.shape[1]\n",
    "    hidden_size = 2\n",
    "    output_size = 1\n",
    "\n",
    "    # Инициализация весов случайными значениями\n",
    "    W1 = np.random.randn(hidden_size, input_size)\n",
    "    b1 = np.random.randn(hidden_size)\n",
    "    W2 = np.random.randn(output_size, hidden_size)\n",
    "    b2 = np.random.randn(output_size)\n",
    "\n",
    "\n",
    "    loss_history = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Прямой проход\n",
    "        Y_pred, H, Z1 = forward_pass(X, W1, b1, W2, b2)\n",
    "\n",
    "        # Вычисление ошибки\n",
    "        current_loss = loss(Y_pred, Y)\n",
    "        loss_history.append(current_loss)\n",
    "\n",
    "        # Обратный проход и обновление весов\n",
    "        W1, b1, W2, b2 = backward_pass(X, Y_pred, Y, H, Z1, W1, b1, W2, b2, lr)\n",
    "\n",
    "        if epoch % (epochs/10) == 0:\n",
    "            print(f\"Эпоха {epoch}/{epochs}, Ошибка: {current_loss:.4f}\")\n",
    "\n",
    "    return W1, b1, W2, b2, loss_history\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Данные\n",
    "    X = np.array([\n",
    "        [0, 0, 0],\n",
    "        [0, 0, 1],\n",
    "        [0, 1, 0],\n",
    "        [0, 1, 1],\n",
    "        [1, 0, 0],\n",
    "        [1, 0, 1],\n",
    "        [1, 1, 0],\n",
    "        [1, 1, 1]\n",
    "    ])\n",
    "\n",
    "    Y = np.array([\n",
    "       [0],\n",
    "        [0],\n",
    "        [0],\n",
    "        [1],\n",
    "        [1],\n",
    "        [1],\n",
    "        [1],\n",
    "        [0]\n",
    "    ])\n",
    "    # Обучение\n",
    "    W1_trained, b1_trained, W2_trained, b2_trained, loss_history = train(X, Y, lr=0.1, epochs=10000)\n",
    "\n",
    "    print(\"\\nОбучение завершено!\")\n",
    "    print(\"Веса первого слоя:\\n\", W1_trained)\n",
    "    print(\"Смещения первого слоя:\\n\", b1_trained)\n",
    "    print(\"Веса второго слоя:\\n\", W2_trained)\n",
    "    print(\"Смещения второго слоя:\\n\", b2_trained)\n",
    "\n",
    "    # Тестирование\n",
    "    Y_pred, _, _ = forward_pass(X, W1_trained, b1_trained, W2_trained, b2_trained)\n",
    "    print(\"\\nПрогнозы:\")\n",
    "    for i in range(len(Y)):\n",
    "        print(f\"Вход: {X[i]}, Выход: {Y_pred[i][0]:.4f}, Ожидаемый выход: {Y[i][0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa1d10a-5d48-4cb6-84ad-8fbc5191a7bd",
   "metadata": {},
   "source": [
    "#Без смещений\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b449b7d-a3aa-4385-9de8-0f0f2e181cc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Эпоха 0/10000, Ошибка: 0.1406\n",
      "Эпоха 1000/10000, Ошибка: 0.0758\n",
      "Эпоха 2000/10000, Ошибка: 0.0687\n",
      "Эпоха 3000/10000, Ошибка: 0.0653\n",
      "Эпоха 4000/10000, Ошибка: 0.0629\n",
      "Эпоха 5000/10000, Ошибка: 0.0611\n",
      "Эпоха 6000/10000, Ошибка: 0.0599\n",
      "Эпоха 7000/10000, Ошибка: 0.0590\n",
      "Эпоха 8000/10000, Ошибка: 0.0583\n",
      "Эпоха 9000/10000, Ошибка: 0.0578\n",
      "\n",
      "Обучение завершено!\n",
      "Веса первого слоя:\n",
      " [[ 4.86841272 -1.67034827  5.22529962]\n",
      " [-5.99591281 -1.86900754 -1.0686304 ]]\n",
      "Веса второго слоя:\n",
      " [[  1.572141   -13.40212512]]\n",
      "\n",
      "Прогнозы:\n",
      "Вход: [0 0 0], Выход: 0.0027, Ожидаемый выход: 0\n",
      "Вход: [0 0 1], Выход: 0.1344, Ожидаемый выход: 0\n",
      "Вход: [0 1 0], Выход: 0.1762, Ожидаемый выход: 0\n",
      "Вход: [0 1 1], Выход: 0.7014, Ожидаемый выход: 1\n",
      "Вход: [1 0 0], Выход: 0.8215, Ожидаемый выход: 1\n",
      "Вход: [1 0 1], Выход: 0.8264, Ожидаемый выход: 1\n",
      "Вход: [1 1 0], Выход: 0.8184, Ожидаемый выход: 1\n",
      "Вход: [1 1 1], Выход: 0.8278, Ожидаемый выход: 0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"Сигмоидальная функция активации.\"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    \"\"\"Производная сигмоиды.\"\"\"\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "def forward_pass(X, W1, W2):\n",
    "    \"\"\"Прямой проход через сеть без смещений.\"\"\"\n",
    "    Z1 = np.dot(X, W1.T)\n",
    "    H = sigmoid(Z1)\n",
    "\n",
    "    Z2 = np.dot(H, W2.T)\n",
    "    Y = sigmoid(Z2)\n",
    "\n",
    "    return Y, H, Z1\n",
    "\n",
    "def loss(y_pred, y_true):\n",
    "    \"\"\"Функция потерь (MSE).\"\"\"\n",
    "    return 0.5 * np.mean((y_pred - y_true)**2)\n",
    "\n",
    "def loss_derivative(y_pred, y_true):\n",
    "    \"\"\"Производная функции потерь (MSE).\"\"\"\n",
    "    return y_pred - y_true\n",
    "\n",
    "def backward_pass(X, Y_pred, Y_true, H, Z1, W1, W2, lr):\n",
    "    \"\"\"Обратный проход и обновление весов без смещений.\"\"\"\n",
    "    # Производные\n",
    "    d_L_d_Y = loss_derivative(Y_pred, Y_true)\n",
    "    d_Y_d_Z2 = sigmoid_derivative(np.dot(H, W2.T))\n",
    "    d_Z2_d_W2 = H\n",
    "\n",
    "    d_L_d_Z2 = d_L_d_Y * d_Y_d_Z2\n",
    "    d_L_d_W2 = np.dot(d_L_d_Z2.T, d_Z2_d_W2)\n",
    "\n",
    "\n",
    "    d_H_d_Z1 = sigmoid_derivative(Z1) # Исправлено\n",
    "    d_Z1_d_W1 = X\n",
    "\n",
    "    d_Z2_d_H = W2\n",
    "    d_L_d_H = np.dot(d_L_d_Z2, d_Z2_d_H)\n",
    "    d_L_d_Z1 = d_L_d_H * d_H_d_Z1\n",
    "    d_L_d_W1 = np.dot(d_L_d_Z1.T, d_Z1_d_W1)\n",
    "\n",
    "\n",
    "    # Обновление весов (без смещений)\n",
    "    W1 -= lr * d_L_d_W1\n",
    "    W2 -= lr * d_L_d_W2\n",
    "\n",
    "    return W1, W2\n",
    "\n",
    "\n",
    "def train(X, Y, lr = 0.01, epochs = 10000):\n",
    "    \"\"\"Обучение персептрона без смещений.\"\"\"\n",
    "    input_size = X.shape[1]\n",
    "    hidden_size = 2\n",
    "    output_size = 1\n",
    "\n",
    "    # Инициализация весов случайными значениями\n",
    "    W1 = np.random.randn(hidden_size, input_size)\n",
    "    W2 = np.random.randn(output_size, hidden_size)\n",
    "\n",
    "\n",
    "    loss_history = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Прямой проход\n",
    "        Y_pred, H, Z1 = forward_pass(X, W1, W2)\n",
    "\n",
    "        # Вычисление ошибки\n",
    "        current_loss = loss(Y_pred, Y)\n",
    "        loss_history.append(current_loss)\n",
    "\n",
    "        # Обратный проход и обновление весов\n",
    "        W1, W2 = backward_pass(X, Y_pred, Y, H, Z1, W1, W2, lr)\n",
    "\n",
    "        if epoch % (epochs/10) == 0:\n",
    "            print(f\"Эпоха {epoch}/{epochs}, Ошибка: {current_loss:.4f}\")\n",
    "\n",
    "    return W1, W2, loss_history\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Данные\n",
    "    X = np.array([\n",
    "        [0, 0, 0],\n",
    "        [0, 0, 1],\n",
    "        [0, 1, 0],\n",
    "        [0, 1, 1],\n",
    "        [1, 0, 0],\n",
    "        [1, 0, 1],\n",
    "        [1, 1, 0],\n",
    "        [1, 1, 1]\n",
    "    ])\n",
    "\n",
    "    Y = np.array([\n",
    "       [0],\n",
    "        [0],\n",
    "        [0],\n",
    "        [1],\n",
    "        [1],\n",
    "        [1],\n",
    "        [1],\n",
    "        [0]\n",
    "    ])\n",
    "    # Обучение\n",
    "    W1_trained, W2_trained, loss_history = train(X, Y, lr=0.1, epochs=10000)\n",
    "\n",
    "    print(\"\\nОбучение завершено!\")\n",
    "    print(\"Веса первого слоя:\\n\", W1_trained)\n",
    "    print(\"Веса второго слоя:\\n\", W2_trained)\n",
    "\n",
    "\n",
    "    # Тестирование\n",
    "    Y_pred, _, _ = forward_pass(X, W1_trained, W2_trained)\n",
    "    print(\"\\nПрогнозы:\")\n",
    "    for i in range(len(Y)):\n",
    "        print(f\"Вход: {X[i]}, Выход: {Y_pred[i][0]:.4f}, Ожидаемый выход: {Y[i][0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c85b36-6cdd-4102-b6ae-169591821911",
   "metadata": {},
   "source": [
    "#Со встроенными библотеками"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e2451f65-7de0-4541-a46e-b532a3befb87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Обученные веса скрытого слоя:\n",
      "[[-0.03267398  2.13886257]\n",
      " [ 0.61693506  0.23828005]\n",
      " [-0.53852984 -0.27396641]]\n",
      "\n",
      "Обученные веса выходного слоя:\n",
      "[[-0.47971112]\n",
      " [ 1.01512545]]\n",
      "\n",
      "Обученные смещения скрытого слоя:\n",
      "[-0.41154359  0.20911454]\n",
      "\n",
      "Обученные смещения выходного слоя:\n",
      "[-1.46175893]\n",
      "\n",
      "Проверка персептрона:\n",
      "Input: [0 0 0], Predicted: 0, Target: 0\n",
      "Input: [0 0 1], Predicted: 0, Target: 0\n",
      "Input: [0 1 0], Predicted: 0, Target: 0\n",
      "Input: [0 1 1], Predicted: 0, Target: 1\n",
      "Input: [1 0 0], Predicted: 1, Target: 1\n",
      "Input: [1 0 1], Predicted: 1, Target: 1\n",
      "Input: [1 1 0], Predicted: 1, Target: 1\n",
      "Input: [1 1 1], Predicted: 1, Target: 0\n",
      "\n",
      "Среднеквадратичная ошибка (MSE): 0.25\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import mean_squared_error\n",
    "# Входные данные и целевые значения\n",
    "inputs = np.array([[0, 0, 0],\n",
    "                   [0, 0, 1],\n",
    "                   [0, 1, 0],\n",
    "                   [0, 1, 1],\n",
    "                   [1, 0, 0],\n",
    "                   [1, 0, 1],\n",
    "                   [1, 1, 0],\n",
    "                   [1, 1, 1]])\n",
    "targets = np.array([0, 0, 0, 1, 1, 1, 1, 0])\n",
    "\n",
    "# Создание и обучение многослойного персептрона (MLP)\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(2,), \n",
    "                    activation='identity', #logistic-сишмоида, identity - без изменений\n",
    "                    solver='sgd', \n",
    "                    learning_rate_init=0.5, \n",
    "                    max_iter=100000, \n",
    "                    random_state=42,\n",
    "                    shuffle=False, # Отключаем перемешивание\n",
    "                    )\n",
    "mlp.fit(inputs, targets)\n",
    "\n",
    "# Извлечение весов\n",
    "weights_hidden = mlp.coefs_[0]\n",
    "weights_output = mlp.coefs_[1]\n",
    "#смещения\n",
    "biases_hidden = mlp.intercepts_[0]\n",
    "biases_output = mlp.intercepts_[1]\n",
    "# Вывод обученных весов\n",
    "print(\"Обученные веса скрытого слоя:\")\n",
    "print(weights_hidden)\n",
    "print(\"\\nОбученные веса выходного слоя:\")\n",
    "print(weights_output)\n",
    "print(\"\\nОбученные смещения скрытого слоя:\")\n",
    "print(biases_hidden)\n",
    "print(\"\\nОбученные смещения выходного слоя:\")\n",
    "print(biases_output)\n",
    "\n",
    "# Проверка персептрона\n",
    "print(\"\\nПроверка персептрона:\")\n",
    "predictions = mlp.predict(inputs)\n",
    "for i in range(len(inputs)):\n",
    "    print(f\"Input: {inputs[i]}, Predicted: {predictions[i]}, Target: {targets[i]}\")\n",
    "mse = mean_squared_error(targets, predictions)\n",
    "print(f\"\\nСреднеквадратичная ошибка (MSE): {mse}\")\n",
    "#сомнительный код"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a72fa1d5-10f9-4eff-b3a4-b7f49b70a4ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Владимир\\venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 791ms/step - accuracy: 0.3750 - loss: 0.7142\n",
      "Epoch 2/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.3750 - loss: 0.7141\n",
      "Epoch 3/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.3750 - loss: 0.7139\n",
      "Epoch 4/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.3750 - loss: 0.7138\n",
      "Epoch 5/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.3750 - loss: 0.7136\n",
      "Epoch 6/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.3750 - loss: 0.7135\n",
      "Epoch 7/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.3750 - loss: 0.7133\n",
      "Epoch 8/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.5000 - loss: 0.7132\n",
      "Epoch 9/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.5000 - loss: 0.7130\n",
      "Epoch 10/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.5000 - loss: 0.7129\n",
      "Epoch 11/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.5000 - loss: 0.7127\n",
      "Epoch 12/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.5000 - loss: 0.7126\n",
      "Epoch 13/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.5000 - loss: 0.7125\n",
      "Epoch 14/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - accuracy: 0.5000 - loss: 0.7123\n",
      "Epoch 15/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.5000 - loss: 0.7122\n",
      "Epoch 16/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.6250 - loss: 0.7121\n",
      "Epoch 17/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.6250 - loss: 0.7119\n",
      "Epoch 18/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.6250 - loss: 0.7118\n",
      "Epoch 19/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.5000 - loss: 0.7117\n",
      "Epoch 20/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.5000 - loss: 0.7116\n",
      "Epoch 21/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.5000 - loss: 0.7114\n",
      "Epoch 22/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.5000 - loss: 0.7113\n",
      "Epoch 23/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.5000 - loss: 0.7112\n",
      "Epoch 24/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.5000 - loss: 0.7111\n",
      "Epoch 25/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.5000 - loss: 0.7110\n",
      "Epoch 26/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.5000 - loss: 0.7109\n",
      "Epoch 27/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.5000 - loss: 0.7107\n",
      "Epoch 28/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.5000 - loss: 0.7106\n",
      "Epoch 29/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.5000 - loss: 0.7105\n",
      "Epoch 30/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.5000 - loss: 0.7104\n",
      "Epoch 31/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 0.5000 - loss: 0.7103\n",
      "Epoch 32/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 0.5000 - loss: 0.7102\n",
      "Epoch 33/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.5000 - loss: 0.7101\n",
      "Epoch 34/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.5000 - loss: 0.7100\n",
      "Epoch 35/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.5000 - loss: 0.7099\n",
      "Epoch 36/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.5000 - loss: 0.7098\n",
      "Epoch 37/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.5000 - loss: 0.7097\n",
      "Epoch 38/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.5000 - loss: 0.7096\n",
      "Epoch 39/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.5000 - loss: 0.7095\n",
      "Epoch 40/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.5000 - loss: 0.7094\n",
      "Epoch 41/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.5000 - loss: 0.7093\n",
      "Epoch 42/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.5000 - loss: 0.7092\n",
      "Epoch 43/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.5000 - loss: 0.7091\n",
      "Epoch 44/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.5000 - loss: 0.7090\n",
      "Epoch 45/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.5000 - loss: 0.7090\n",
      "Epoch 46/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.5000 - loss: 0.7089\n",
      "Epoch 47/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.5000 - loss: 0.7088\n",
      "Epoch 48/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.5000 - loss: 0.7087\n",
      "Epoch 49/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - accuracy: 0.5000 - loss: 0.7086\n",
      "Epoch 50/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.5000 - loss: 0.7085\n",
      "Epoch 51/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.5000 - loss: 0.7084\n",
      "Epoch 52/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.5000 - loss: 0.7084\n",
      "Epoch 53/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.5000 - loss: 0.7083\n",
      "Epoch 54/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.5000 - loss: 0.7082\n",
      "Epoch 55/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.5000 - loss: 0.7081\n",
      "Epoch 56/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.5000 - loss: 0.7080\n",
      "Epoch 57/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.5000 - loss: 0.7079\n",
      "Epoch 58/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 0.5000 - loss: 0.7079\n",
      "Epoch 59/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.5000 - loss: 0.7078\n",
      "Epoch 60/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.5000 - loss: 0.7077\n",
      "Epoch 61/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.5000 - loss: 0.7076\n",
      "Epoch 62/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.5000 - loss: 0.7075\n",
      "Epoch 63/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - accuracy: 0.5000 - loss: 0.7075\n",
      "Epoch 64/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.5000 - loss: 0.7074\n",
      "Epoch 65/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.5000 - loss: 0.7073\n",
      "Epoch 66/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.5000 - loss: 0.7072\n",
      "Epoch 67/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.5000 - loss: 0.7071\n",
      "Epoch 68/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.5000 - loss: 0.7071\n",
      "Epoch 69/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.5000 - loss: 0.7070\n",
      "Epoch 70/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.5000 - loss: 0.7069\n",
      "Epoch 71/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.5000 - loss: 0.7068\n",
      "Epoch 72/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.5000 - loss: 0.7068\n",
      "Epoch 73/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.5000 - loss: 0.7067\n",
      "Epoch 74/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.5000 - loss: 0.7066\n",
      "Epoch 75/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.5000 - loss: 0.7065\n",
      "Epoch 76/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step - accuracy: 0.5000 - loss: 0.7065\n",
      "Epoch 77/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.5000 - loss: 0.7064\n",
      "Epoch 78/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.5000 - loss: 0.7063\n",
      "Epoch 79/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.5000 - loss: 0.7062\n",
      "Epoch 80/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.5000 - loss: 0.7061\n",
      "Epoch 81/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.5000 - loss: 0.7061\n",
      "Epoch 82/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.5000 - loss: 0.7060\n",
      "Epoch 83/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.5000 - loss: 0.7059\n",
      "Epoch 84/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.5000 - loss: 0.7058\n",
      "Epoch 85/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step - accuracy: 0.5000 - loss: 0.7058\n",
      "Epoch 86/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.5000 - loss: 0.7057\n",
      "Epoch 87/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.5000 - loss: 0.7056\n",
      "Epoch 88/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.5000 - loss: 0.7055\n",
      "Epoch 89/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.5000 - loss: 0.7055\n",
      "Epoch 90/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.5000 - loss: 0.7054\n",
      "Epoch 91/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.5000 - loss: 0.7053\n",
      "Epoch 92/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.5000 - loss: 0.7052\n",
      "Epoch 93/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.5000 - loss: 0.7052\n",
      "Epoch 94/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.5000 - loss: 0.7051\n",
      "Epoch 95/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.5000 - loss: 0.7050\n",
      "Epoch 96/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.5000 - loss: 0.7049\n",
      "Epoch 97/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.5000 - loss: 0.7049\n",
      "Epoch 98/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - accuracy: 0.5000 - loss: 0.7048\n",
      "Epoch 99/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.5000 - loss: 0.7047\n",
      "Epoch 100/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.5000 - loss: 0.7046\n",
      "Обученные веса скрытого слоя:\n",
      "[[-0.3890944  -0.6920778 ]\n",
      " [-0.19328423  0.4862602 ]\n",
      " [ 0.9667514  -0.01032386]]\n",
      "\n",
      "Обученные смещения скрытого слоя (установлены в 0):\n",
      "[-0.41154359  0.20911454]\n",
      "\n",
      "Обученные веса выходного слоя:\n",
      "[[-0.6959644]\n",
      " [ 0.8684326]]\n",
      "\n",
      "Обученные смещения выходного слоя (установлены в 0):\n",
      "[-1.46175893]\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x0000015098198860> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "\n",
      "Проверка персептрона:\n",
      "Input: [0 0 0], Predicted: 1.0, Target: 0\n",
      "Input: [0 0 1], Predicted: 0.0, Target: 0\n",
      "Input: [0 1 0], Predicted: 1.0, Target: 0\n",
      "Input: [0 1 1], Predicted: 1.0, Target: 1\n",
      "Input: [1 0 0], Predicted: 0.0, Target: 1\n",
      "Input: [1 0 1], Predicted: 0.0, Target: 1\n",
      "Input: [1 1 0], Predicted: 1.0, Target: 1\n",
      "Input: [1 1 1], Predicted: 0.0, Target: 0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Входные данные и целевые значения\n",
    "inputs = np.array([[0, 0, 0],\n",
    "                   [0, 0, 1],\n",
    "                   [0, 1, 0],\n",
    "                   [0, 1, 1],\n",
    "                   [1, 0, 0],\n",
    "                   [1, 0, 1],\n",
    "                   [1, 1, 0],\n",
    "                   [1, 1, 1]])\n",
    "targets = np.array([0, 0, 0, 1, 1, 1, 1, 0])\n",
    "\n",
    "# Преобразование данных в тензоры\n",
    "inputs_tensor = tf.convert_to_tensor(inputs, dtype=tf.float32)\n",
    "targets_tensor = tf.convert_to_tensor(targets, dtype=tf.int32)\n",
    "\n",
    "\n",
    "# Создание модели\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(2, activation='sigmoid', use_bias=True, input_shape=(3,)), # Без смещений\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid', use_bias=True) # Без смещений\n",
    "])\n",
    "\n",
    "\n",
    "# Компиляция модели\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Обучение модели\n",
    "model.fit(inputs_tensor, targets_tensor, epochs=100, verbose=1)\n",
    "\n",
    "\n",
    "# Вывод весов\n",
    "weights_hidden = model.layers[0].get_weights()[0]\n",
    "# biases_hidden = np.zeros(shape=(2,)) # Смещения для скрытого слоя\n",
    "weights_output = model.layers[1].get_weights()[0]\n",
    "# biases_output = np.zeros(shape=(1,)) # Смещения для выходного слоя\n",
    "\n",
    "\n",
    "print(\"Обученные веса скрытого слоя:\")\n",
    "print(weights_hidden)\n",
    "print(\"\\nОбученные смещения скрытого слоя (установлены в 0):\")\n",
    "print(biases_hidden)\n",
    "print(\"\\nОбученные веса выходного слоя:\")\n",
    "print(weights_output)\n",
    "print(\"\\nОбученные смещения выходного слоя (установлены в 0):\")\n",
    "print(biases_output)\n",
    "\n",
    "\n",
    "# Проверка персептрона\n",
    "predictions = model.predict(inputs_tensor).round()\n",
    "print(\"\\nПроверка персептрона:\")\n",
    "for i in range(len(inputs)):\n",
    "        print(f\"Input: {inputs[i]}, Predicted: {(predictions[i][0])}, Target: {targets[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f29a1a-2076-4e1b-9c4b-73f00dd0f20a",
   "metadata": {},
   "source": [
    "#В классе"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f4c30299-7d9c-46d0-9db4-b709de6a941f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d: 0, 0, 0, 1, 1, 1, 1, 0;\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 79\u001b[0m\n\u001b[0;32m     77\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-------------------------------\u001b[39m\u001b[38;5;124m\"\u001b[39m, count)\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;28mprint\u001b[39m()\n\u001b[1;32m---> 79\u001b[0m \u001b[43mbin_func3_neuron_test\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[28], line 66\u001b[0m, in \u001b[0;36mbin_func3_neuron_test\u001b[1;34m()\u001b[0m\n\u001b[0;32m     64\u001b[0m y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;241m8\u001b[39m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mfloat\u001b[39m)\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(x)):\n\u001b[1;32m---> 66\u001b[0m     y[i] \u001b[38;5;241m=\u001b[39m \u001b[43mn4\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactivate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mn1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactivate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactivate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn3\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactivate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m NeuronMath\u001b[38;5;241m.\u001b[39mcompare_arrays(y, d) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw1:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mstr\u001b[39m(v) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m n1\u001b[38;5;241m.\u001b[39mweights), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m;    \u001b[39m\u001b[38;5;124m\"\u001b[39m, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[28], line 11\u001b[0m, in \u001b[0;36mNeuronMath.activate\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mactivate\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs):\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx \u001b[38;5;241m=\u001b[39m inputs\n\u001b[1;32m---> 11\u001b[0m     weighted_sum \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweights\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Crucial!\u001b[39;00m\n\u001b[0;32m     12\u001b[0m     weighted_sum \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m weighted_sum \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class NeuronMath:\n",
    "    def __init__(self, weights):\n",
    "        self.weights = weights\n",
    "        self.x = None  # Входные данные\n",
    "        self.y = None  # Выход\n",
    "\n",
    "    def activate(self, inputs):\n",
    "        self.x = inputs\n",
    "        weighted_sum = np.dot(inputs, self.weights[:-1])  # Crucial!\n",
    "        weighted_sum += self.weights[-1]\n",
    "        self.y = 1 if weighted_sum >= 0 else 0\n",
    "        return self.y\n",
    "\n",
    "    @staticmethod\n",
    "    def next_value(val, min_val, max_val, step):\n",
    "        for i in range(len(val) - 1, -1, -1):\n",
    "            if val[i] < max_val[i]:\n",
    "                val[i] += step[i]\n",
    "                for j in range(i + 1, len(val)):\n",
    "                    val[j] = min_val[j]\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    @staticmethod\n",
    "    def compare_arrays(arr1, arr2):\n",
    "        for i in range(min(len(arr1), len(arr2))):\n",
    "            if arr1[i] < arr2[i]:\n",
    "                return -1\n",
    "            elif arr1[i] > arr2[i]:\n",
    "                return 1\n",
    "        if len(arr1) < len(arr2):\n",
    "            return -1\n",
    "        elif len(arr1) > len(arr2):\n",
    "            return 1\n",
    "        return 0\n",
    "def bin_func3_neuron_test():\n",
    "    x = np.array([\n",
    "        [0, 0, 0], [0, 0, 1], [0, 1, 0], [0, 1, 1],\n",
    "        [1, 0, 0], [1, 0, 1], [1, 1, 0], [1, 1, 1]\n",
    "    ], dtype=float)\n",
    "    \n",
    "    d = np.array([0, 0, 0, 1, 1, 1, 1, 0], dtype=float)\n",
    "    min_val = np.array([-1, -1, -1, -1.5, -1, -1, -1, -1.5, 0, 0, 0, 0, -1, -1, 0, -2.5], dtype=float)\n",
    "    max_val = np.array([1, 1, 1, 1.5, 1, 1, 1, 1.5, 0, 0, 0, 0, 1, 1, 0, 2.5], dtype=float)\n",
    "    val = np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=float)\n",
    "    step = np.array([.5, .5, .5, .5, .5, .5, .5, .5, .5, .5, .5, .5, .5, .5, .5, .5], dtype=float)\n",
    "    \n",
    "    n1 = NeuronMath(np.zeros(4, dtype=float))\n",
    "    n2 = NeuronMath(np.zeros(4, dtype=float))\n",
    "    n3 = NeuronMath(np.zeros(4, dtype=float))\n",
    "    n4 = NeuronMath(np.zeros(4, dtype=float))\n",
    "    \n",
    "    print(\"d:\", \", \".join(str(int(i)) for i in d) + \";\")\n",
    "    \n",
    "    count = 0\n",
    "    while NeuronMath.next_value(val, min_val, max_val, step):\n",
    "        n1.weights = val[0:4].copy()\n",
    "        n2.weights = val[4:8].copy()\n",
    "        n3.weights = val[8:12].copy()\n",
    "        n4.weights = val[12:16].copy()\n",
    "        \n",
    "        y = np.zeros(8, dtype=float)\n",
    "        for i in range(len(x)):\n",
    "            y[i] = n4.activate([n1.activate(x[i]), n2.activate(x[i]), n3.activate(x[i])])\n",
    "    \n",
    "        if NeuronMath.compare_arrays(y, d) == 0:\n",
    "            print(\"w1:\", \", \".join(str(v) for v in n1.weights), \";    \", end=\"\")\n",
    "            print(\"w2:\", \", \".join(str(v) for v in n2.weights), \";    \", end=\"\")\n",
    "            print(\"w3:\", \", \".join(str(v) for v in n3.weights), \";    \", end=\"\")\n",
    "            print(\"w4:\", \", \".join(str(v) for v in n4.weights), \";    \")\n",
    "            count += 1\n",
    "            for i in range(len(x)):\n",
    "                y_i = n4.activate([n1.activate(x[i]), n2.activate(x[i]), n3.activate(x[i])])\n",
    "                print(f\"x[{int(x[i][0])},{int(x[i][1])},{int(x[i][2])}] -> [{int(n1.y)},{int(n2.y)},{int(n3.y)}] -> {int(y_i)}\")\n",
    "    print(\"-------------------------------\", count)\n",
    "    print()\n",
    "bin_func3_neuron_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6dd992-5643-4a9e-ab53-34561755dc8a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
