3)Цель: Наша цель - минимизировать ошибку классификации. Мы хотим подобрать такие веса w, чтобы разделяющая гиперплоскость (прямая) максимально точно разделяла данные на классы.

Что такое градиент? Градиент - это вектор, который указывает направление наискорейшего роста функции. В нашем случае функция - это функция ошибки, и мы хотим двигаться в противоположном направлении (минимизировать ошибку), поэтому мы берем антиградиент.

Как считается градиент: В классическом варианте вычисляется производная функции ошибки по всем параметрам модели. В нашем случае это:

Производная ошибки по весам w (w[j]): определяет, как изменение w[j] повлияет на ошибку классификации.

Производная ошибки по свободному члену w (w[-1])

В методе стохастического градиента (SGD) считается не общий градиент, а градиент для каждой точки.

В чем суть gradSum[j] += 0 if s * d > 0 else -d * p[j]:

Это выражение — это оценка или приближение антиградиента функции ошибки по весу w[j], для текущей точки данных. Мы не вычисляем производную формально, а выводим ее, исходя из того, что надо сделать, если точка классифицирована неверно.
Если точка классифицирована верно (s*d > 0), то производная ошибки по весу w[j] (влияние w[j] на ошибку) считается 0 (вес корректировать не нужно).
Если точка классифицирована неверно (s*d <= 0), то производная ошибки по весу w[j] (влияние w[j] на ошибку) оценивается, исходя из принципа: нужно двигать веса так, чтобы классифицировать точку верно. А именно, если:
Точка с меткой +1 классифицируется неверно, значит s < 0, надо увеличить значение s. Мы делаем это за счет увеличения веса, соответствующего положительному признаку, и уменьшения веса, соответствующего отрицательному. В выражении -d * p[j], если d положительно, то мы уменьшаем соответствующий вес.
Точка с меткой -1 классифицируется неверно, значит s > 0, надо уменьшить значение s. Мы делаем это за счет уменьшения веса, соответствующего положительному признаку, и увеличения веса, соответствующего отрицательному. В выражении -d * p[j], если d отрицательно, то мы увеличиваем соответствующий вес.
Значение p[j] определяет насколько нужно увеличить или уменьшить вес w[j]. Если значение признака большое, то и корректировка должна быть больше.
Почему используется p[j]?
p[j] является признаком точки. Когда мы корректируем вес w[j], мы это делаем пропорционально значению p[j]. То есть, чем больше значение признака, тем большее изменение мы должны внести в соответствующий вес.

В чем смысл: Мы не вычисляем производные формально, мы “предполагаем” как надо изменить веса, если точка классифицирована неверно. Это не очень формально, но этого оказывается достаточно, чтобы получить приемлемые результаты.